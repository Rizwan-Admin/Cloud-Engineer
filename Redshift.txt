#Amazon Redshift

#Data base, Data lake , data wearhouse, data lakehouse

#(S3, HDFS ) Data lake ----> Data wearhouse ( Redshift)
#Data lake , mo need to what format of data and schema
	
#Qustion: can you vizulize data from data lake , can--> but data homogenise required
----------------------------------------------------------------------------

extact or ingest-- load              
                   Storage         
                   Data lake --- Data Wearhouse

        Kafka---------------------
        Sqoop-----  Hdfs  ------- Hive
        flume---------------------
        kinesis---  S3  -------- Redshif 

================================================
OLTP-- Data Base --
OLAP -- Data Wearhouse --
=======================
Hadoop --  Cloudera CDP 
  *Cloudera Directore for enterprice 3to5year run in Market 

Cloud-- AWS , Azure, GCP
=============
Redshifr: OLAP
>>Ability to have parallel processing 
>>MPP --- Qury excution engine whic all to you parell processing
>> Fully Manage
>> cluster Sizing are admin responibility
>> Maintian the Cluster

Question : What happend Worker Node or Data Node Down?	
         issue power fail, etc
Anwser: On prem . de-comisson
        cloud -- new instance lauch --add node--then --balance -then -- de commison


>>Redshoft provide automatic detecte , it will provission new node, itwiil replicate data, then terminate old not, 
		this is not auto scalling
        redshift does not have auto scalling.auto scalling is differnt
		thsi is auto matic recovery.
		
>> capacity planing will be do, no need to take cluster plaining.
=======================================================

*Tabluse coonect hive via Hive-serve
*query editor : hude brwoser


#Architecure 

Client connect via JDBC/ OBDCC/-->> leaader node



        ----------------leader node------------
     compute node      compute node           compute node


Leader node Recive the query
leadr node:it will developed excution paln--then trensfer compute node
leader node: brake the task and assign compute node and compute node they back to the result. then leader node aggregate the result.


user data store in compute node.

#question:how many max node in redshift
answer: maximum 128 node support

Aws Responsible up and runing leader node. becasue its manage service.

-------------------------------------------


# Quetion :why, as a Hadoop administrator why are we giving so much important NN?
  anwser: Master Node maintain responbility. 
  hadoopadmin responsible up and runing NN.
------------------------------------------

Use Caase Redshift
Global sales data.
Historical trade data -- used by Nasdaq

web clicking for Advertise
gaming: user , access partent agregeted

social media : for trend analys

healtcare
----------------------------------------------

Data Sourse-->S3-->EMR-->S3-->Redshif-->QuikSight

data sourse-->s3--> Redshist Spectrum--

Data source-->s3--> glue ---s3 --- Redshift

visulation before distiled data where is load? ans: redshift.



---------------------------------------------

Raw data --- process-- Distaild Data
Redshift Petabyte Scale
Emr deploy for processing.

Amazon file cache --- is aws service
----------------------

Data Analytics: Redshift

Administration and Manage is our responbility 
Opration and Suppot.
sample data load-- for POC 

Data query and  load is not aover responsbility.

Hbase: coulmner data

RedshiftL Culmner data

===========================
*Data engineer is responsible for data load in organization
but its not diffecult.

Redshit data load  -- click on load Data-s3 url--

*Redshift Block Size = 1 mb
*Hadoop Block Size = 128 mb

*Redshift take only structure data , semi structure data transfer then load.
compression : column based

*sequencial based data, column based data achive processing fast in Redshift.


suppose drive fail

Redshift problem: no auto scalling
dc2 instance

compute node: 1> deep strorage  2> deep compute.

deep storage is leggacy now.

if you have old  work load so idealy is better to migrate dc2 or RA3 work load.

Question:what about incoming data while creating cluster from snapshoot.

scalling concept :
 first about maintenance window creat if no have auto scalling, then nw cluster create, exiting cluster read opration and new cluster create, and change C name in Route 54 then data parelle transfer.



=============================
calaster planing :
====================

#Redshift:
Monitaring -- aws cloud wacth
Audting -- aws cloud trail
Encrytion -- aws kms

========================
EMR:transite cluster concept :provission then trminate thhen next day new cluster deploy in emr .

Redshift : cluster puse concept -->. not transite cluster,
           no data lost
		   cost optimization
		   
*data is available for stackholder. is our responsibility.
======================

#aws cloud HSM--> Recomanded new cluster
usually plaining time
================================
in Redshift:-->
vaccume : responsible for data engineer. but some time Admin do,so maintenace window time we do.

=============================

ELK Stack -- tradtional technology


tablue integrate : HS2 integrate we required tablue  driver and url .

tablue admin ensure tablue is up and runing.
................................

	 